---
title: "Class prediction using random forest model"
author: "AngieMolina"
date: "7/07/2020"
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Overview
Today its very common for people to use devices and take measurements regularly to quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, the goal is to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).

The data for this project and some assumptions used come from this source: http://groupware.les.inf.puc-rio.br/har.

Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.

## Basic exploratory and tidy data analysis
For understanding the data, the dataset is loaded, read and visualized. 

```{r warning=FALSE, message=FALSE}
library (caret)
library (randomForest)
library(rpart)
```

```{r echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE}
## Create a local directory for the data used for the project
lD <- "predictionExercise"
if (!file.exists(lD)) {
  dir.create(lD)
}

##Download the data from the webpage
trainFileUrl <-
  "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testFileUrl <-
  "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

file <- "pml-training.csv"
if (!file.exists(file)) {
  download.file(trainFileUrl, file)
}

file1 <- "pml-testing.csv"
if (!file.exists(file1)) {
  download.file(testFileUrl, file1)
}

## Read the data and assign it to a dataframe
trainData <-
  read.csv(file,
           as.is = TRUE,
           na.strings = c("NA", "#DIV/0!", ""))
testData <-
  read.csv(file1,
           as.is = TRUE,
           na.strings = c("NA", "#DIV/0!", "")) 
```

```{r warning=FALSE, message=FALSE}
print (c("Train data:", dim(trainData)))
print (c("Test data:", dim(testData)))
##Tidy data, remove Nas and select variables for analysis
trainD <- trainData[, colSums(is.na(trainData)) == 0]
test <- testData[, colSums(is.na(testData)) == 0]
trainD <- trainD[, -c(1:7)]
test <- test[, -c(1:7)]
```
The training dataframe has 19622 observations and 160 variables. The test data has 20 observations and 160 variables. From both data sets de Nas and associated columns are removed. Finally, taking into account the article, the first 7 columns are removed since they don't contain information that contribute to this analysis. 

## Model selection
Once the data sets are organized, the training data is divided in 2 parts, one for training the model and the other for validation (will be used for crossvalidation purposes). In this case since I don't know which is the best model, I will try 2 models with high accuracy for this type of data. For this selection I take the validation set and divide it in two sets one for training and the other for validation.

```{r warning=FALSE, message=FALSE}
set.seed(12345)
inTrain <- createDataPartition(trainD$classe, p = 3 / 4, list = FALSE)
trainD <- trainD[inTrain, ]
testV <- trainD[-inTrain, ]
print (c("Train data:", dim(trainD)))
print (c("Validation data:", dim(testV)))
print (c("Test data:", dim(test)))

##Create a dataset for evaluating models
inTrainE <- createDataPartition(testV$classe, p = 3 / 4, list = FALSE)
trainDE <- testV[inTrainE, ]
testVE <- testV[-inTrainE, ]

##Try 2 different models with the new data for model selection
modelRfE <- train(classe ~ ., data = trainDE, method = "rf")
modelGbmE <-
  train(classe ~ .,
        data = trainDE,
        method = "gbm",
        verbose = FALSE)

predictRfE <- predict(modelRfE, testVE)
predictGbmE <- predict(modelGbmE, testVE)

cMrf <-
  confusionMatrix(table(predictRfE, testVE$classe))$overall['Accuracy']
cMgbm <-
  confusionMatrix(table(predictGbmE, testVE$classe))$overall['Accuracy']

print (c("Random forest accuracy:", round(cMrf, 4)))
print (c("Gradient boosting accuracy:", round(cMgbm,4)))
```

The accuracy(sensibility) results for the random forest model was 0.96, and for the gradient boosting accuracy was 0.927. Then, I select the random forest model for this project. 

## Prediction results
Once the model is selected, the original training data and vaidation set are used. For this final model a 5 fold cross validation is used. 

```{r warning=FALSE, message=FALSE}
set.seed(56789)
ctrl = trainControl(method="cv", 5)
modelFit <-
  train(classe ~ .,
        data = trainD,
        method = "rf",
        ntree = 250)
modelFit


predictV <- predict(modelFit, testV)
confusionMatrix(table(predictV, testV$classe))

plot(modelFit$finalModel, main = "Model fit random forest")
plot(predictV,
     xlab = "Classes",
     ylab = "Count",
     main = "Prediction with random forest model")
```

The results show an accuracy of 1 using the model. As the first plot shows, the data is classified in 5 classes, having more data for class A. The second plot shows the error asociated to the amount of trees used. As its shown the error tends to be 0 as the aumount of trees increase, which explain why the accuracy obtained is 1. In this case the out of sample error is the difference of accuracy (1-accuracy) so it tends to be 0. 

## Test data prediction
Once the selected model was evaluated with the cross validation, the test data is predicted.
```{r warning=FALSE, message=FALSE}
predictT <- predict(modelFit, test)
predictT
```
